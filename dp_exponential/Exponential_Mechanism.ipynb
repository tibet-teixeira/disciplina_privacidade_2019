{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy.stats import expon\n",
    "from collections import Counter\n",
    "import operator\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "COLUMN NAME - INDEX\n",
    "gross           8\n",
    "movie_title    11\n",
    "language       19\n",
    "country        20\n",
    "'''\n",
    "def loadDataset(filename):\n",
    "    dataset = pd.read_csv(filename, delimiter=',')\n",
    "    return np.array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Color', 'James Cameron', 723.0, ..., 7.9, 1.78, 33000],\n",
       "       ['Color', 'Gore Verbinski', 302.0, ..., 7.1, 2.35, 0],\n",
       "       ['Color', 'Sam Mendes', 602.0, ..., 6.8, 2.35, 85000],\n",
       "       ...,\n",
       "       ['Color', 'Benjamin Roberds', 13.0, ..., 6.3, nan, 16],\n",
       "       ['Color', 'Daniel Hsia', 14.0, ..., 6.3, 2.35, 660],\n",
       "       ['Color', 'Jon Gunn', 43.0, ..., 6.6, 1.85, 456]], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG\n",
    "datasetTest = loadDataset('movie_metadata.csv')\n",
    "datasetTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clearing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove line limite  \n",
    "def clearingDatasetQuery1(dataset):\n",
    "    subset = dataset[0:100, [8, 11]]\n",
    "    new_dataset = list()\n",
    "    for record in subset:\n",
    "        if not(math.isnan(record[0])):\n",
    "            new_dataset.append(record)\n",
    "    return np.array(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove line limite  \n",
    "def clearingDatasetQuery2(dataset):   \n",
    "    subset = dataset[300:400, [8, 11, 19]]\n",
    "    new_dataset = list()\n",
    "    for record in subset:\n",
    "        if not(math.isnan(record[0])):\n",
    "            new_dataset.append(record)\n",
    "    return np.array(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove line limite  \n",
    "def clearingDatasetQuery3(dataset):   \n",
    "    subset = dataset[300:400, [20, 11]]\n",
    "    new_dataset = list()\n",
    "    for record in subset:\n",
    "        if not(record[1] == 'NaN'):\n",
    "            new_dataset.append(record)\n",
    "\n",
    "    return np.array(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def possibleValues(dataset, query):  \n",
    "    if query == 1:\n",
    "        cleanDataset = clearingDatasetQuery1(dataset)\n",
    "    elif query == 2:\n",
    "        cleanDataset = clearingDatasetQuery2(dataset)\n",
    "    elif query == 3:\n",
    "        cleanDataset = clearingDatasetQuery3(dataset)\n",
    "    else:\n",
    "        print('Consulta inválida')\n",
    "        return None\n",
    "       \n",
    "    indexRecord = 0\n",
    "    titleUniqueMovies = list()\n",
    "    removingReplicates = np.copy(cleanDataset)\n",
    "    \n",
    "    for record in cleanDataset:\n",
    "        if record[1] not in titleUniqueMovies:\n",
    "            titleUniqueMovies.append(record[1])\n",
    "        else:\n",
    "            removingReplicates = np.delete(removingReplicates, indexRecord, 0)\n",
    "            indexRecord -= 1\n",
    "            \n",
    "        indexRecord += 1\n",
    "       \n",
    "    return np.array(removingReplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "datasetQuery1 = possibleValues(datasetTest, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "datasetQuery2 = possibleValues(datasetTest, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "datasetQuery3 = possibleValues(datasetTest, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataset(dataset):\n",
    "    movieDict = dict()\n",
    "    maxGrossPerLanguage = dict()\n",
    "    \n",
    "    for record in dataset:\n",
    "        if record[2] in movieDict.keys():\n",
    "            movieDict[record[2]].append(record)\n",
    "        else:\n",
    "            movieDict[record[2]] = []\n",
    "            movieDict[record[2]].append(record)\n",
    "    return movieDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query1(dataset):\n",
    "    maxGross = np.max(dataset[:, 0], axis=0)\n",
    "    return dataset[np.where(dataset[:,0] == maxGross)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query2(dataset):\n",
    "    movieDict = dict()\n",
    "    maxGrossPerLanguage = dict()\n",
    "    \n",
    "    for record in dataset:\n",
    "        if record[2] in movieDict.keys():\n",
    "            movieDict[record[2]].append(record)\n",
    "        else:\n",
    "            movieDict[record[2]] = []\n",
    "            movieDict[record[2]].append(record)\n",
    "        \n",
    "    for key in movieDict.keys():\n",
    "        maxGross = float('-inf')\n",
    "        movieName = None\n",
    "        for value in movieDict[key]:\n",
    "            if value[0] > maxGross:\n",
    "                maxGross = value[0]\n",
    "                movieName = value[1]\n",
    "\n",
    "        maxGrossPerLanguage[key] = (maxGross, movieName)\n",
    "\n",
    "    return maxGrossPerLanguage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query3(dataset):\n",
    "    countOccurrences = Counter(dataset[:,0])\n",
    "    countOccurrencesSorted = sorted(countOccurrences.items(), \n",
    "                                    key = lambda kv:(kv[1], kv[0]), \n",
    "                                    reverse=True)[:3]\n",
    "    \n",
    "    return dict((language, occurrences) for language, occurrences in countOccurrencesSorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([760505847.0, 'Avatar\\xa0'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG\n",
    "query1(datasetQuery1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'English': (380838870.0, 'Finding Nemo\\xa0'),\n",
       " 'Mandarin': (9213.0, 'The Flowers of War\\xa0'),\n",
       " 'Aboriginal': (72515360.0, 'The Interpreter\\xa0')}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG\n",
    "query2(datasetQuery2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'USA': 88, 'UK': 5, 'Germany': 4}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG\n",
    "query3(datasetQuery3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreFunctionQuery1(dataset, output):\n",
    "    score = list()\n",
    "       \n",
    "    for record in dataset:       \n",
    "        if record[1] == output:\n",
    "            score.append(record[0])\n",
    "        else:\n",
    "            score.append(0)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreFunctionQuery2(dataset, output):\n",
    "    score = list()\n",
    "       \n",
    "    for record in dataset:       \n",
    "        if record[1] == output:\n",
    "            score.append(record[0])\n",
    "        else:\n",
    "            score.append(0)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Como será calculado?\n",
    "def scoreFunctionQuery3(dataset, output):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivityQuery1(dataset):\n",
    "    datasetWithoutCurrentOutput = dataset[:,:]\n",
    "    \n",
    "    maxScore = float('-inf')\n",
    "    indexOutput = 0\n",
    "    \n",
    "    for output in dataset:\n",
    "        lenDataset = len(datasetWithoutCurrentOutput)\n",
    "        datasetWithoutCurrentOutput = datasetWithoutCurrentOutput[np.delete(np.array(range(lenDataset)), 0),:]\n",
    "        neighborDataset = datasetWithoutCurrentOutput[:,:]\n",
    "        \n",
    "        scoreQ1 = scoreFunctionQuery1(dataset, output[1])\n",
    "        maxScoreQ1 = np.max(scoreQ1)\n",
    "        minScoreQ1 = float('inf')\n",
    "    \n",
    "        for index in range(lenDataset - 1):\n",
    "            lenNeighborDataset = len(neighborDataset)\n",
    "            neighborDataset = neighborDataset[np.delete(np.array(range(lenNeighborDataset)), 0),:]\n",
    "            \n",
    "            for newOutput in neighborDataset:\n",
    "                newScoreQ1 = scoreFunctionQuery1(neighborDataset, newOutput[1])\n",
    "                minScoreNeighbor = np.min(newScoreQ1)\n",
    "\n",
    "            if minScoreQ1 > minScoreNeighbor:\n",
    "                minScoreQ1 = minScoreNeighbor \n",
    "                ScoreDifference = abs(maxScoreQ1 - minScoreQ1)\n",
    "\n",
    "                if maxScore < ScoreDifference:\n",
    "                    maxScore = ScoreDifference\n",
    "        \n",
    "        indexOutput += 1\n",
    "    \n",
    "    return maxScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Esta função deve calcular a sensibilidade de cada saída para o dataset de seu idioma.\n",
    "# Saídas de idiomas diferentes não fazem parte do dataset, portanto, é necessário dividir o dataset\n",
    "# por idioma e a partir disso calcular a sensibilidade\n",
    "\n",
    "# TODO: Rever os casos para um unico filme de um idioma. \n",
    "# O dataset vizinho é vazio e portanto o valor da sensibilidade é -inf\n",
    "\n",
    "def sensitivityQuery2(dataset):   \n",
    "    datasetPerLanguage = splitDataset(dataset)\n",
    "    maxScorePerLanguage = dict()\n",
    "    \n",
    "    for language in datasetPerLanguage.keys():\n",
    "        maxScore = float('-inf')\n",
    "        indexOutput = 0\n",
    "        newDataset = list()\n",
    "        \n",
    "        for record in datasetPerLanguage[language]:\n",
    "            newDataset.append(record)\n",
    "        \n",
    "        newDataset = np.array(newDataset)\n",
    "        datasetWithoutCurrentOutput = newDataset[:,:]\n",
    "\n",
    "        for output in newDataset:\n",
    "            lenDataset = len(datasetWithoutCurrentOutput)\n",
    "            datasetWithoutCurrentOutput = datasetWithoutCurrentOutput[np.delete(np.array(range(lenDataset)), 0),:]\n",
    "            neighborDataset = datasetWithoutCurrentOutput[:,:]\n",
    "\n",
    "            scoreQ2 = scoreFunctionQuery2(newDataset, output[1])\n",
    "\n",
    "            maxScoreQ2 = np.max(scoreQ2)\n",
    "            minScoreQ2 = float('inf')\n",
    "\n",
    "            for index in range(lenDataset - 1):\n",
    "                lenNeighborDataset = len(neighborDataset)\n",
    "                neighborDataset = neighborDataset[np.delete(np.array(range(lenNeighborDataset)), 0),:]\n",
    "\n",
    "                for newOutput in neighborDataset:\n",
    "                    newScoreQ2 = scoreFunctionQuery2(neighborDataset, newOutput[1])\n",
    "                    minScoreNeighbor = np.min(newScoreQ2)\n",
    "\n",
    "                if minScoreQ2 > minScoreNeighbor:\n",
    "                    minScoreQ2 = minScoreNeighbor \n",
    "                    ScoreDifference = abs(maxScoreQ2 - minScoreQ2)\n",
    "\n",
    "                    if maxScore < ScoreDifference:\n",
    "                        maxScore = ScoreDifference\n",
    "            \n",
    "            indexOutput += 1\n",
    "        if maxScore == float('-inf'):\n",
    "            maxScore = maxScoreQ2\n",
    "            \n",
    "        maxScorePerLanguage[language] = maxScore\n",
    "    \n",
    "    return maxScorePerLanguage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivityQuery3(dataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "760505847.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG\n",
    "sensQ1 = sensitivityQuery1(datasetQuery1)\n",
    "sensQ1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'English': 380838870.0, 'Mandarin': 9213.0, 'Aboriginal': 72515360.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG\n",
    "sensQ2 = sensitivityQuery2(datasetQuery2)\n",
    "sensQ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "sensQ3 = sensitivityQuery3(datasetQuery3)\n",
    "sensQ3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputRandomized(budget, sensitivity):\n",
    "    exp = expon.rvs(loc = 0, scale = sensitivity/budget)\n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomizedQuery1(dataset, budget, sensitivity):\n",
    "    grosses = dataset[:, 0]\n",
    "    values = list()\n",
    "    for gross in grosses:\n",
    "        value = abs(gross - outputRandomized(budget, sensitivity))\n",
    "        values.append(value)\n",
    "\n",
    "    return dataset[np.argmin(values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomizedQuery2(dataset, budget, sensitivity):\n",
    "    datasetPerLanguage = splitDataset(dataset)\n",
    "    grossPerLanguage = dict()\n",
    "    \n",
    "    for language in datasetPerLanguage.keys():\n",
    "        newDataset = list()\n",
    "        \n",
    "        for record in datasetPerLanguage[language]:\n",
    "            newDataset.append(record)\n",
    "        \n",
    "        newDataset = np.array(newDataset)\n",
    "         \n",
    "        grosses = newDataset[:, 0]\n",
    "        values = list()\n",
    "        \n",
    "        for gross in grosses:\n",
    "            value = abs(gross - outputRandomized(budget, sensitivity[language]))\n",
    "            values.append(value)\n",
    "            \n",
    "        grossPerLanguage[language] = newDataset[np.argmin(values)]\n",
    "\n",
    "    return grossPerLanguage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomizedQuery3(dataset, budget, sensitivity):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([258355354.0, 'The Hobbit: The Desolation of Smaug\\xa0'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG\n",
    "randomizedQuery1(datasetQuery1, 0.1, sensQ1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'English': array([78747585.0, 'Pixels\\xa0', 'English'], dtype=object),\n",
       " 'Mandarin': array([9213.0, 'The Flowers of War\\xa0', 'Mandarin'], dtype=object),\n",
       " 'Aboriginal': array([72515360.0, 'The Interpreter\\xa0', 'Aboriginal'], dtype=object)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG\n",
    "randomizedQuery2(datasetQuery2, 0.1, sensQ2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "randomizedQuery3(datasetQuery3, 0.1, sensQ3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(filename, budget):\n",
    "    dataset = loadDataset(filename)\n",
    "#     cleanDataset = possibleValues(datasetTest)\n",
    "    \n",
    "#     sensitivityQ1 = sensitivityQuery1(cleanDataset)\n",
    "#     result = randomizedQuery1(cleanDataset, budget, sensitivityQ1)\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('movie_metadata.csv', [])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
