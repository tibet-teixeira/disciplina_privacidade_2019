{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy.stats import rv_discrete\n",
    "from collections import Counter\n",
    "import operator\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "COLUMN NAME - INDEX\n",
    "gross           8\n",
    "movie_title    11\n",
    "language       19\n",
    "country        20\n",
    "'''\n",
    "def loadDataset(filename):\n",
    "    dataset = pd.read_csv(filename, delimiter=',')\n",
    "    return np.array(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clearing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove line limite  \n",
    "def clearingDatasetQuery1(dataset):\n",
    "    subset = dataset[0:100, [8, 11]]\n",
    "    new_dataset = list()\n",
    "    for record in subset:\n",
    "        if not(math.isnan(record[0])):\n",
    "            new_dataset.append(record)\n",
    "    return np.array(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove line limite  \n",
    "def clearingDatasetQuery2(dataset):   \n",
    "    subset = dataset[300:400, [8, 11, 19]]\n",
    "    new_dataset = list()\n",
    "    for record in subset:\n",
    "        if not(math.isnan(record[0])):\n",
    "            new_dataset.append(record)\n",
    "    return np.array(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove line limite  \n",
    "def clearingDatasetQuery3(dataset):   \n",
    "    subset = dataset[300:400, [20, 11]]\n",
    "    new_dataset = list()\n",
    "    for record in subset:\n",
    "        if not(record[1] == 'NaN'):\n",
    "            new_dataset.append(record)\n",
    "\n",
    "    new_dataset = np.array(new_dataset)\n",
    "\n",
    "    countOccurrences = Counter(new_dataset[:,0])\n",
    "    countOccurrencesSorted = sorted(countOccurrences.items(), \n",
    "                                    key = lambda kv:(kv[1], kv[0]), \n",
    "                                    reverse=True)\n",
    "    \n",
    "\n",
    "    newDataset = list()\n",
    "    languages = list()\n",
    "    counts = list()\n",
    "    \n",
    "    for language, count in countOccurrencesSorted:\n",
    "        languages.append(language)\n",
    "        counts.append(count)\n",
    "    \n",
    "    newDataset = np.array(pd.DataFrame({'language':languages, 'count':counts}))\n",
    "    \n",
    "    return newDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def possibleValues(dataset, query):  \n",
    "    if query == 1:\n",
    "        cleanDataset = clearingDatasetQuery1(dataset)\n",
    "    elif query == 2:\n",
    "        cleanDataset = clearingDatasetQuery2(dataset)\n",
    "    else:\n",
    "        print('Consulta invÃ¡lida')\n",
    "        return None\n",
    "       \n",
    "    indexRecord = 0\n",
    "    titleUniqueMovies = list()\n",
    "    removingReplicates = np.copy(cleanDataset)\n",
    "    \n",
    "    for record in cleanDataset:\n",
    "        if record[1] not in titleUniqueMovies:\n",
    "            titleUniqueMovies.append(record[1])\n",
    "        else:\n",
    "            removingReplicates = np.delete(removingReplicates, indexRecord, 0)\n",
    "            indexRecord -= 1\n",
    "            \n",
    "        indexRecord += 1\n",
    "       \n",
    "    return np.array(removingReplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataset(dataset):\n",
    "    movieDict = dict()\n",
    "    maxGrossPerLanguage = dict()\n",
    "    \n",
    "    for record in dataset:\n",
    "        if record[2] in movieDict.keys():\n",
    "            movieDict[record[2]].append(record)\n",
    "        else:\n",
    "            movieDict[record[2]] = []\n",
    "            movieDict[record[2]].append(record)\n",
    "    return movieDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query1(dataset):\n",
    "    maxGross = np.max(dataset[:, 0], axis=0)\n",
    "    return dataset[np.where(dataset[:,0] == maxGross)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query2(dataset):\n",
    "    movieDict = dict()\n",
    "    maxGrossPerLanguage = dict()\n",
    "    \n",
    "    for record in dataset:\n",
    "        if record[2] in movieDict.keys():\n",
    "            movieDict[record[2]].append(record)\n",
    "        else:\n",
    "            movieDict[record[2]] = []\n",
    "            movieDict[record[2]].append(record)\n",
    "        \n",
    "    for key in movieDict.keys():\n",
    "        maxGross = float('-inf')\n",
    "        movieName = None\n",
    "        for value in movieDict[key]:\n",
    "            if value[0] > maxGross:\n",
    "                maxGross = value[0]\n",
    "                movieName = value[1]\n",
    "\n",
    "        maxGrossPerLanguage[key] = (maxGross, movieName)\n",
    "\n",
    "    return maxGrossPerLanguage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query3(dataset):\n",
    "    top3 = dataset[:3]\n",
    "    \n",
    "    return dict((language, occurrences) for language, occurrences in top3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreFunctionQuery1(dataset, output):\n",
    "    score = list()\n",
    "       \n",
    "    for record in dataset:       \n",
    "        if record[1] == output:\n",
    "            score.append(record[0])\n",
    "        else:\n",
    "            score.append(0)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreFunctionQuery2(dataset, output):\n",
    "    score = list()\n",
    "       \n",
    "    for record in dataset:       \n",
    "        if record[1] == output:\n",
    "            score.append(record[0])\n",
    "        else:\n",
    "            score.append(0)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreFunctionQuery3(dataset, output):\n",
    "    score = list()\n",
    "       \n",
    "    for record in dataset:       \n",
    "        if record[0] == output:\n",
    "            score.append(record[1])\n",
    "        else:\n",
    "            score.append(0)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivityQuery1(dataset):\n",
    "    datasetWithoutCurrentOutput = dataset[:,:]\n",
    "    \n",
    "    maxScore = float('-inf')\n",
    "    indexOutput = 0\n",
    "    \n",
    "    for output in dataset:\n",
    "        lenDataset = len(datasetWithoutCurrentOutput)\n",
    "        datasetWithoutCurrentOutput = datasetWithoutCurrentOutput[np.delete(np.array(range(lenDataset)), 0),:]\n",
    "        neighborDataset = datasetWithoutCurrentOutput[:,:]\n",
    "        \n",
    "        scoreQ1 = scoreFunctionQuery1(dataset, output[1])\n",
    "        maxScoreQ1 = np.max(scoreQ1)\n",
    "        minScoreQ1 = float('inf')\n",
    "    \n",
    "        for index in range(lenDataset - 1):\n",
    "            lenNeighborDataset = len(neighborDataset)\n",
    "            neighborDataset = neighborDataset[np.delete(np.array(range(lenNeighborDataset)), 0),:]\n",
    "            \n",
    "            for newOutput in neighborDataset:\n",
    "                newScoreQ1 = scoreFunctionQuery1(neighborDataset, newOutput[1])\n",
    "                minScoreNeighbor = np.min(newScoreQ1)\n",
    "\n",
    "                if minScoreQ1 > minScoreNeighbor:\n",
    "                    minScoreQ1 = minScoreNeighbor \n",
    "                    ScoreDifference = abs(maxScoreQ1 - minScoreQ1)\n",
    "\n",
    "                    if maxScore < ScoreDifference:\n",
    "                        maxScore = ScoreDifference\n",
    "        \n",
    "        indexOutput += 1\n",
    "    \n",
    "    return maxScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivityQuery2(dataset):   \n",
    "    datasetPerLanguage = splitDataset(dataset)\n",
    "    maxScorePerLanguage = dict()\n",
    "    \n",
    "    for language in datasetPerLanguage.keys():\n",
    "        maxScore = float('-inf')\n",
    "        indexOutput = 0\n",
    "        newDataset = list()\n",
    "        \n",
    "        for record in datasetPerLanguage[language]:\n",
    "            newDataset.append(record)\n",
    "        \n",
    "        newDataset = np.array(newDataset)\n",
    "        datasetWithoutCurrentOutput = newDataset[:,:]\n",
    "\n",
    "        for output in newDataset:\n",
    "            lenDataset = len(datasetWithoutCurrentOutput)\n",
    "            datasetWithoutCurrentOutput = datasetWithoutCurrentOutput[np.delete(np.array(range(lenDataset)), 0),:]\n",
    "            neighborDataset = datasetWithoutCurrentOutput[:,:]\n",
    "\n",
    "            scoreQ2 = scoreFunctionQuery2(newDataset, output[1])\n",
    "\n",
    "            maxScoreQ2 = np.max(scoreQ2)\n",
    "            minScoreQ2 = float('inf')\n",
    "\n",
    "            for index in range(lenDataset - 1):\n",
    "                lenNeighborDataset = len(neighborDataset)\n",
    "                neighborDataset = neighborDataset[np.delete(np.array(range(lenNeighborDataset)), 0),:]\n",
    "\n",
    "                for newOutput in neighborDataset:\n",
    "                    newScoreQ2 = scoreFunctionQuery2(neighborDataset, newOutput[1])\n",
    "                    minScoreNeighbor = np.min(newScoreQ2)\n",
    "\n",
    "                    if minScoreQ2 > minScoreNeighbor:\n",
    "                        minScoreQ2 = minScoreNeighbor \n",
    "                        ScoreDifference = abs(maxScoreQ2 - minScoreQ2)\n",
    "\n",
    "                        if maxScore < ScoreDifference:\n",
    "                            maxScore = ScoreDifference\n",
    "            \n",
    "            indexOutput += 1\n",
    "        if maxScore == float('-inf'):\n",
    "            maxScore = maxScoreQ2\n",
    "            \n",
    "        maxScorePerLanguage[language] = maxScore\n",
    "    \n",
    "    return maxScorePerLanguage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivityQuery3(dataset):\n",
    "    datasetWithoutCurrentOutput = dataset[:,:]\n",
    "    \n",
    "    maxScore = float('-inf')\n",
    "    indexOutput = 0\n",
    "    \n",
    "    for output in dataset:\n",
    "        lenDataset = len(datasetWithoutCurrentOutput)\n",
    "        datasetWithoutCurrentOutput = datasetWithoutCurrentOutput[np.delete(np.array(range(lenDataset)), 0),:]\n",
    "        neighborDataset = datasetWithoutCurrentOutput[:,:]\n",
    "        \n",
    "        scoreQ3 = scoreFunctionQuery3(dataset, output[0])\n",
    "        maxScoreQ3 = np.max(scoreQ3)\n",
    "        \n",
    "        minScoreQ3 = float('inf')\n",
    "    \n",
    "        for index in range(lenDataset - 1):\n",
    "            lenNeighborDataset = len(neighborDataset)\n",
    "            neighborDataset = neighborDataset[np.delete(np.array(range(lenNeighborDataset)), 0),:]\n",
    "            \n",
    "            for newOutput in neighborDataset:\n",
    "                newScoreQ3 = scoreFunctionQuery3(neighborDataset, newOutput[0])\n",
    "                minScoreNeighbor = np.min(newScoreQ3)\n",
    "\n",
    "                if minScoreQ3 > minScoreNeighbor:\n",
    "                    minScoreQ3 = minScoreNeighbor \n",
    "                    ScoreDifference = abs(maxScoreQ3 - minScoreQ3)\n",
    "\n",
    "                    if maxScore < ScoreDifference:\n",
    "                        maxScore = ScoreDifference\n",
    "        \n",
    "        indexOutput += 1\n",
    "    \n",
    "    return maxScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateExp(score, budget, sensitivity):\n",
    "    return math.exp((budget * score) / (2 * sensitivity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateOutputProbabilityQuery1(dataset, budget, sensitivity):\n",
    "    expList = list()\n",
    "    probabilityList = list()\n",
    "    \n",
    "    for output in dataset:\n",
    "        score = np.max(scoreFunctionQuery1(dataset, output[1]))\n",
    "        expList.append(calculateExp(score, budget, sensitivity))\n",
    "        \n",
    "    sumExp = np.sum(expList)\n",
    "    \n",
    "    for value in expList:\n",
    "        probabilityList.append(value/sumExp)\n",
    "        \n",
    "    return probabilityList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateOutputProbabilityQuery2(dataset, language, budget, sensitivity):\n",
    "    expList = list()\n",
    "    probabilityList = list()\n",
    "    \n",
    "    for output in dataset:        \n",
    "        score = np.max(scoreFunctionQuery2(dataset, output[1]))\n",
    "        expList.append(calculateExp(score, budget, sensitivity[language]))\n",
    "\n",
    "    sumExp = np.sum(expList)\n",
    "    \n",
    "    for value in expList:\n",
    "        probabilityList.append(value/sumExp)\n",
    "        \n",
    "    return probabilityList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateOutputProbabilityQuery3(dataset, budget, sensitivity):\n",
    "    expList = list()\n",
    "    probabilityList = list()\n",
    "    \n",
    "    for output in dataset:\n",
    "        score = np.max(scoreFunctionQuery3(dataset, output[0]))\n",
    "        expList.append(calculateExp(score, budget, sensitivity))\n",
    "        \n",
    "    sumExp = np.sum(expList)\n",
    "    \n",
    "    for value in expList:\n",
    "        probabilityList.append(value/sumExp)\n",
    "        \n",
    "    return probabilityList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomizedQuery1(dataset, budget, sensitivity):\n",
    "    lenDataset = len(dataset)\n",
    "    listProbabilities = calculateOutputProbabilityQuery1(dataset, budget, sensitivity)\n",
    "    distribution = rv_discrete(values = (np.array(range(lenDataset)) , listProbabilities))\n",
    "    return dataset[distribution.rvs(size=1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomizedQuery2(dataset, budget, sensitivity):\n",
    "    datasetPerLanguage = splitDataset(dataset)\n",
    "    grossPerLanguage = dict()\n",
    "    \n",
    "    for language in datasetPerLanguage.keys():\n",
    "        newDataset = list()\n",
    "        \n",
    "        for record in datasetPerLanguage[language]:\n",
    "            newDataset.append(record)\n",
    "        \n",
    "        newDataset = np.array(newDataset)\n",
    "        \n",
    "        lenDataset = len(newDataset)\n",
    "        listProbabilities = calculateOutputProbabilityQuery2(newDataset, language, budget, sensitivity)\n",
    "        distribution = rv_discrete(values = (np.array(range(lenDataset)) , listProbabilities))\n",
    "        grossPerLanguage[language] = newDataset[distribution.rvs(size=1), :]\n",
    "        \n",
    "    return grossPerLanguage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomizedQuery3(dataset, budget, sensitivity):\n",
    "    budget = budget/3\n",
    "    \n",
    "    rankedList = list()\n",
    "    lenDataset = len(dataset)\n",
    "    \n",
    "    for rank in range(3):        \n",
    "        listProbabilities = calculateOutputProbabilityQuery3(dataset, budget, sensitivity)\n",
    "        distribution = rv_discrete(values = (np.array(range(lenDataset)) , listProbabilities))\n",
    "        indexOutput = distribution.rvs(size=1)\n",
    "        rankedList.append(dataset[indexOutput, :])\n",
    "                \n",
    "        dataset = dataset[np.delete(np.array(range(lenDataset)), indexOutput),:]\n",
    "        lenDataset = len(dataset)\n",
    "        \n",
    "    return rankedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(filename, budgets):\n",
    "    dataset = loadDataset(filename)\n",
    "    \n",
    "    datasetQuery1 = possibleValues(dataset, 1)\n",
    "    datasetQuery2 = possibleValues(dataset, 2)\n",
    "    datasetQuery3 = clearingDatasetQuery3(dataset)\n",
    "    \n",
    "    q1 = query1(datasetQuery1)\n",
    "    q2 = query2(datasetQuery2)\n",
    "    q3 = query3(datasetQuery3)\n",
    "    \n",
    "    sensQ1 = sensitivityQuery1(datasetQuery1)\n",
    "    sensQ2 = sensitivityQuery2(datasetQuery2)\n",
    "    sensQ3 = sensitivityQuery3(datasetQuery3)\n",
    "    \n",
    "    for budget in budgets:\n",
    "        \n",
    "        randomized1 = randomizedQuery1(datasetQuery1, budget, sensQ1)\n",
    "        randomized2 = randomizedQuery2(datasetQuery2, budget, sensQ2)\n",
    "        randomized3 = randomizedQuery3(datasetQuery3, budget, sensQ3)\n",
    "        \n",
    "        print('Budget {} \\n'.format(budget))\n",
    "        \n",
    "        print('Original query 1 = {} '.format(q1))\n",
    "        print('Randomized query 1 = {} '.format(randomized1))\n",
    "        print('Sensitivity query 1 = {} '.format(sensQ1))\n",
    "        print('\\n')\n",
    "        \n",
    "        print('Original query 2 = {} '.format(q2))\n",
    "        print('Randomized query 2 = {} '.format(randomized2))\n",
    "        print('Sensitivity query 2 = {} '.format(sensQ2))\n",
    "        print('\\n')\n",
    "        \n",
    "        print('Original query 3 = {} '.format(q3))\n",
    "        print('Randomized query 3 = {} '.format(randomized3))\n",
    "        print('Sensitivity query 3 = {} '.format(sensQ3))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Budget 0.1 \n",
      "\n",
      "Original query 1 = [760505847.0 'Avatar\\xa0'] \n",
      "Randomized query 1 = [[234903076.0 'Oz the Great and Powerful\\xa0']] \n",
      "Sensitivity query 1 = 760505847.0 \n",
      "\n",
      "\n",
      "Original query 2 = {'English': (380838870.0, 'Finding Nemo\\xa0'), 'Mandarin': (9213.0, 'The Flowers of War\\xa0'), 'Aboriginal': (72515360.0, 'The Interpreter\\xa0')} \n",
      "Randomized query 2 = {'English': array([[34964818.0, 'Pan\\xa0', 'English']], dtype=object), 'Mandarin': array([[9213.0, 'The Flowers of War\\xa0', 'Mandarin']], dtype=object), 'Aboriginal': array([[72515360.0, 'The Interpreter\\xa0', 'Aboriginal']], dtype=object)} \n",
      "Sensitivity query 2 = {'English': 380838870.0, 'Mandarin': 9213.0, 'Aboriginal': 72515360.0} \n",
      "\n",
      "\n",
      "Original query 3 = {'USA': 88, 'UK': 5, 'Germany': 4} \n",
      "Randomized query 3 = [array([['UK', 5]], dtype=object), array([['France', 2]], dtype=object), array([['Germany', 4]], dtype=object)] \n",
      "Sensitivity query 3 = 88 \n",
      "\n",
      "\n",
      "Budget 1 \n",
      "\n",
      "Original query 1 = [760505847.0 'Avatar\\xa0'] \n",
      "Randomized query 1 = [[90755643.0 'Prince of Persia: The Sands of Time\\xa0']] \n",
      "Sensitivity query 1 = 760505847.0 \n",
      "\n",
      "\n",
      "Original query 2 = {'English': (380838870.0, 'Finding Nemo\\xa0'), 'Mandarin': (9213.0, 'The Flowers of War\\xa0'), 'Aboriginal': (72515360.0, 'The Interpreter\\xa0')} \n",
      "Randomized query 2 = {'English': array([[78747585.0, 'Pixels\\xa0', 'English']], dtype=object), 'Mandarin': array([[9213.0, 'The Flowers of War\\xa0', 'Mandarin']], dtype=object), 'Aboriginal': array([[72515360.0, 'The Interpreter\\xa0', 'Aboriginal']], dtype=object)} \n",
      "Sensitivity query 2 = {'English': 380838870.0, 'Mandarin': 9213.0, 'Aboriginal': 72515360.0} \n",
      "\n",
      "\n",
      "Original query 3 = {'USA': 88, 'UK': 5, 'Germany': 4} \n",
      "Randomized query 3 = [array([['France', 2]], dtype=object), array([['Germany', 4]], dtype=object), array([['UK', 5]], dtype=object)] \n",
      "Sensitivity query 3 = 88 \n",
      "\n",
      "\n",
      "Budget 10 \n",
      "\n",
      "Original query 1 = [760505847.0 'Avatar\\xa0'] \n",
      "Randomized query 1 = [[262030663.0 'The Amazing Spider-Man\\xa0']] \n",
      "Sensitivity query 1 = 760505847.0 \n",
      "\n",
      "\n",
      "Original query 2 = {'English': (380838870.0, 'Finding Nemo\\xa0'), 'Mandarin': (9213.0, 'The Flowers of War\\xa0'), 'Aboriginal': (72515360.0, 'The Interpreter\\xa0')} \n",
      "Randomized query 2 = {'English': array([[377019252.0, 'The Lord of the Rings: The Return of the King\\xa0',\n",
      "        'English']], dtype=object), 'Mandarin': array([[9213.0, 'The Flowers of War\\xa0', 'Mandarin']], dtype=object), 'Aboriginal': array([[72515360.0, 'The Interpreter\\xa0', 'Aboriginal']], dtype=object)} \n",
      "Sensitivity query 2 = {'English': 380838870.0, 'Mandarin': 9213.0, 'Aboriginal': 72515360.0} \n",
      "\n",
      "\n",
      "Original query 3 = {'USA': 88, 'UK': 5, 'Germany': 4} \n",
      "Randomized query 3 = [array([['USA', 88]], dtype=object), array([['Germany', 4]], dtype=object), array([['China', 1]], dtype=object)] \n",
      "Sensitivity query 3 = 88 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main('movie_metadata.csv', [0.1, 1, 10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
